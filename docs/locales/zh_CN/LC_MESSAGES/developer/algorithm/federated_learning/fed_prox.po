# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-09-14 17:52+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.10.3\n"

#: ../../developer/algorithm/federated_learning/fed_prox.md:1
msgid "Strategy: FedProx"
msgstr "联邦策略：FedProx"

#: ../../developer/algorithm/federated_learning/fed_prox.md:2
msgid "FedAvg v.s FedProx"
msgstr "FedAvg v.s FedProx"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "Heterogeneity"
msgstr "异构性"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "FedAvg"
msgstr "FedAvg"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "FedProx"
msgstr "FedProx"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "data heterogeneity"
msgstr "数据异构性"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "FedAvg does not guarantee fit for non-iid data"
msgstr "FedAvg不能对non-iid数据保证拟合"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid ""
"FedProx can guarantee the fitting rate for non-iid data: by changing the "
"objective function of the overall optimization, based on an assumption of"
" distribution difference, a convergence proof is obtained"
msgstr "FedProx能够对non-iid数据保证拟合速率"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "device heterogeneity"
msgstr "设备异构性"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid ""
"FedAvg does not take into account the heterogeneity of different devices,"
" such as differences in computing power; for each device, the same "
"workload is scheduled"
msgstr "FedAvg没有考虑不同设备的异质性，例如计算力上的不同；对于每个设备来说，会安排相同的workload"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid ""
"FedProx supports local training for each device with different workloads:"
" adjust the accuracy requirements for local training per device by "
"setting a different \"γ-inexact\" parameter for each device in each round"
msgstr ""
"FedProx支持让每个设备进行不同workload的本地训练：通过在每轮为每个设备设置一个不同的“γ "
"-inexact”参数，调整对每个设备在本地训练的精度要求；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:7
msgid "Fit"
msgstr "Fit"

#: ../../developer/algorithm/federated_learning/fed_prox.md:8
msgid ""
"Add Proximal Term to the objective function "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mathop{max}\\limits_{w}h_k(w;w^t)=F_k(w)&plus;\\frac{\\mu}{2}||w-w^t||^2)"
msgstr ""
"在目标函数中增加Proximal Term "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mathop{max}\\limits_{w}h_k(w;w^t)=F_k(w)&plus;\\frac{\\mu}{2}||w-w^t||^2)"

#: ../../developer/algorithm/federated_learning/fed_prox.md:11
msgid ""
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;F_k(w)):"
" For a set of parameters w, the loss obtained by training k local data on"
" device k   "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;w^t):"
" The initial model parameters sent by the server to device k in the t-th "
"round of training   "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu):"
" a hyperparameter   "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu/2||w&space;-&space;w^t||^2&space;):"
" proxy term, which limits the difference between the optimized w and the "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;w^t)"
" released in the t round, so that the updated w of each device k is equal"
" to Do not differ too much between them to help fit"
msgstr ""
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;F_k(w)):"
" "
"对于一组参数w，在设备k上训练k本地数据得到的loss![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;w^t):"
" "
"在第t轮训练中，服务器发送给设备k的初始模型参数![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu):"
" 一个超参数 "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu/2||w&space;-&space;w^t||^2&space;):"
" proximal "
"term，限制优化后的w与![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;w^t)"
" t轮发布的差异度，让每个设备k更新后的w之间不要差别过大，帮助拟合"

#: ../../developer/algorithm/federated_learning/fed_prox.md:15
msgid "Different training requirements for each device: γ-inexact"
msgstr "每个设备不同训练要求：γ -inexact"

#: ../../developer/algorithm/federated_learning/fed_prox.md:17
msgid ""
"![defination_2](resources/fedprox_defination_2.jpg) FedAvg requires that "
"each device is fully optimized for E epochs during training locally; Due to"
" equipment heterogeneity, FedProx hopes to put forward different "
"optimization requirements for each equipment k in each round t, and does "
"not require all equipment to be completely optimized; "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu_k^t&space;\\in&space;[0,1]),"
" the higher the value, the looser the constraints, that is, the lower the"
" training completion requirements for equipment k; on the contrary, when "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu_k^t&space;=&space;0),"
" the parameters are required the training update is 0, requiring the "
"local model to fully fit; With the help of "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu_k^t),"
" the training volume per round of each device can be adjusted according "
"to the computing resources of the device;"
msgstr "FedAvg要求每个设备在本地训练时都完整地进行优化E个epochs；而由于设备异质性，FedProx希望在每一轮t中对每一个设备k提出不同的优化要求，不需要所有设备都完整地进行优化；![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu_k^t&space;\\in&space;[0,1])，值越高代表限制条件越宽松，即对设备k的训练完成度要求越低；反之，当![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu_k^t&space;=&space;0)的时候要求参数训练的更新为0，要求本地模型完全拟合；借助![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu_k^t)可以按照设备的计算力资源调整每个设备每轮的训练量；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:17
msgid "defination_2"
msgstr "defination_2"

#: ../../developer/algorithm/federated_learning/fed_prox.md:22
msgid "Requirements for parameter selection in Convergence analysis"
msgstr "Convergence分析中对参数选取的要求"

#: ../../developer/algorithm/federated_learning/fed_prox.md:23
msgid ""
"When the selected parameters meet the following conditions, the expected "
"value of the convergence rate of the model can be bound"
msgstr "当选取的参数满足下述条件，模型的convergence rate的期望值可以被bound"

#: ../../developer/algorithm/federated_learning/fed_prox.md:24
msgid "Parameter conditions"
msgstr "参数条件"

#: ../../developer/algorithm/federated_learning/fed_prox.md:26
msgid "![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\rho^t=(\\frac{1}{\\mu}-\\frac{\\gamma^tB}{\\mu}-\\frac{B(1&plus;\\gamma^t\\sqrt(2))}{&space;\\overline\\mu\\sqrt(K)}-\\frac{LB(1&plus;\\gamma^t)}{\\overline\\mu\\mu}&space;-&space;\\frac{L(1&plus;\\gamma^t)^2B^2}&space;{2\\mu^2}-\\frac{LB^2(1&plus;\\gamma^t)^2}{\\mu^2K}(2\\sqrt{2K}&plus;2)))"
msgstr "![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\rho^t=(\\frac{1}{\\mu}-\\frac{\\gamma^tB}{\\mu}-\\frac{B(1&plus;\\gamma^t\\sqrt(2))}{&space;\\overline\\mu\\sqrt(K)}-\\frac{LB(1&plus;\\gamma^t)}{\\overline\\mu\\mu}&space;-&space;\\frac{L(1&plus;\\gamma^t)^2B^2}&space;{2\\mu^2}-\\frac{LB^2(1&plus;\\gamma^t)^2}{\\mu^2K}(2\\sqrt{2K}&plus;2)))"

#: ../../developer/algorithm/federated_learning/fed_prox.md:28
msgid ""
"There are three groups of parameters to set: K, "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma),"
" "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu);"
" Where K is the number of client devices selected in the t round, "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma^t=max_k(\\gamma_k^t)),"
" "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma),"
" "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu)"
" is the hyperparameter for setting the proxy term. **B is a value used to"
" assume the upper limit of the current participation data distribution "
"difference, not sure how to get it**"
msgstr ""
"要设置参数有三组：K，![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma)，![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu);其中K是t轮中选取的client设备数量，![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma^t=max_k(\\gamma_k^t)),"
" "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\mu)是设置proximal"
" term的超参数；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:31
msgid "fitting rate"
msgstr "拟合速率"

#: ../../developer/algorithm/federated_learning/fed_prox.md:32
msgid ""
"Convergence can be proven if the parameters are set to meet the above "
"requirements"
msgstr "如果参数的设置满足上述要求，则能够证明收敛性"

#: ../../developer/algorithm/federated_learning/fed_prox.md:34
msgid "Sufficient and non-essential conditions for parameter selection"
msgstr "参数选取的充分非必要条件"

#: ../../developer/algorithm/federated_learning/fed_prox.md:35
msgid ""
"![remark_5](resources/fedprox_remark_5.jpg)   There is a tradeoff between"
" "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma)"
" and "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;B)."
" For example, the larger "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;B) "
"is, the greater the difference in data distribution, the smaller the "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma)"
" must be, and the higher the training requirements for each device;"
msgstr ""
" "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma)和B之间存在tradeoff，比如B越大说明数据分布差异性越大，则![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma)必须越小，对每个设备的训练要求越高；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:35
msgid "remark_5"
msgstr "remark_5"

#: ../../developer/algorithm/federated_learning/fed_prox.md:38
msgid "Experiment 1: Effectiveness of proximal term and inexactness"
msgstr "实验1: proximal term和inexactness的有效性"

#: ../../developer/algorithm/federated_learning/fed_prox.md:39
msgid "![figure_1](resources/fedprox_figure_1.jpg)"
msgstr "![figure_1](resources/fedprox_figure_1.jpg)"

#: ../../developer/algorithm/federated_learning/fed_prox.md:39
msgid "figure_1"
msgstr "figure_1"

#: ../../developer/algorithm/federated_learning/fed_prox.md:40
msgid "Summarize"
msgstr "总结"

#: ../../developer/algorithm/federated_learning/fed_prox.md:41
msgid ""
"The existence of "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma)"
" and "
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;B) "
"is more theoretical, in practice, the workload of the device can be "
"determined directly according to the device resources"
msgstr ""
"![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;\\gamma)"
" "
"和![](https://latex.codecogs.com/svg.image?\\inline&space;\\small&space;B)"
" 的定义和存在还比较理论, 在实践中可以直接按照设备资源确定设备的workload"

#: ../../developer/algorithm/federated_learning/fed_prox.md:43
msgid "Implementation"
msgstr "实现情况"

#: ../../developer/algorithm/federated_learning/fed_prox.md:44
msgid "The proxy term for data non-iid has been implemented;"
msgstr "针对数据non-iid的proximal term已实现"

#: ../../developer/algorithm/federated_learning/fed_prox.md:45
msgid "The inexactness for device heterogeneity needs to be realized;"
msgstr "针对设备异质性的inexactness待实现"

#: ../../developer/algorithm/federated_learning/fed_prox.md:47
msgid "Reference"
msgstr "参考文献"

#: ../../developer/algorithm/federated_learning/fed_prox.md:48
msgid ""
"[Federated Optimization in Heterogeneous "
"Networks](https://arxiv.org/pdf/1812.06127.pdf)"
msgstr ""

