# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-25 11:09+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:9
msgid "将单机模型训练代码迁移 SecretFlow 联邦学习训练代码教程"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:21
msgid "引言"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:24
msgid "背景"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:26
msgid "随着数据隐私问题日益受到重视，并且实际业务场景的需求不断演变，联邦学习作为一种特殊的深度学习形式开始迅速兴起。它以一种创新的方式解决了传统集中式训练的数据隐私问题，能够在保护用户隐私的前提下，有效地训练机器学习模型。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:27
msgid "那么假如现实的业务中已经有了单机模型，应该如何将它进行联邦化呢？为了易用性，我们的隐语在设计之初就希望能够以最低的成本来帮助用户迁移已有的单机模型到联邦模型。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:30
msgid "教程目标以及内容"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:32
msgid "通过本教程的学习： 1. 读者可以快速的将已有的单机模型正确的使用SecretFlow来进行联邦化。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:33
msgid "2. 先通过单机模型完成模型开发，再进行联邦化适配，也是一个比较推荐的开发实践，可以提高联邦模型的开发效率。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:35
msgid ""
"本教程将手把手的带你学习如何将已有的单机模型训练代码迁移到SecretFlow中进行联邦学习。 1. 介绍迁移的整体流程 2. "
"通过案例介绍在Tensorflow作为后端的迁移流程 3. 通过案例介绍在Pytorch作为后端的迁移流程"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:47
msgid "迁移的步骤"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:49
#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1190
msgid ""
"基于 PyTorch 从单机模型到联邦学习模型，主要需要添加或修改以下几部分 - 添加联邦学习中的参与方 - 修改数据集的处理逻辑 - "
"修改模型的继承类 - 根据需要决定是否对 metric 、 optimizer 和 loss fuction 进行包装，并使用包装后的函数"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:51
#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:2047
msgid ""
"基于 TensorFlow 从单机模型到联邦学习模型，主要需要添加或修改以下几部分 - 添加联邦学习中的参与方 - 修改数据集的处理逻辑 - "
"进行模型的封装"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:53
#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:2049
msgid "得益于隐语的封装，使用者不需要自己进行大量的代码编写，只需要调用 Secretflow 中的函数，即可便捷完成模型的定义和使用等操作。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:54
msgid "迁移完成后，不同后端不同的模型都使用一套API来进行\\ ``fit``,\\ ``predict``,\\ ``evaluate``\\ 等等"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:66
msgid "前期数据准备"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:68
msgid "相关文档可以参考相关IO文档"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:70
msgid "下载数据集并解压"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:115
msgid "基于PyTorch的迁移教程"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:118
msgid "模型在 PyTorch 下的单机模型实现"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:120
msgid "首先我们给出单机模式下，基于 PyTorch 定义和训练神经网络模型的过程。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:218
msgid "基于 PyTorch 的隐语联邦学习模型迁移"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:220
msgid ""
"基于 PyTorch 从单机模型到联邦学习模型，主要包含以下步骤： - 添加联邦学习中的参与方 - 修改数据集的处理逻辑 - 修改模型的继承类 -"
" 根据需要决定是否对 metric 、 optimizer 和 loss fuction 进行包装，并使用包装后的函数"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:222
msgid "接下来，我们将结合实际代码具体讲解这些步骤。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:234
msgid "环境设置"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:236
msgid "添加联邦学习中的参与方，并对各个参与方进行初始化"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:333
msgid "封装单机模式下的数据处理逻辑"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:335
msgid ""
"同样地，在联邦学习中我们也需要对数据进行预处理，使之符合模型的输入，所以参考在\\ `SecretFlow 中使用自定义 DataBuilder "
"(Torch)构建 dataset builder "
"<https://www.secretflow.org.cn/docs/secretflow/latest/zh-"
"Hans/tutorial/CustomDataLoaderTorch>`__\\ "
"，我们选择文件夹路径作为参数，并且封装单机模式下的数据处理逻辑，最后返回 "
"(data_set，steps_per_epoch)的结果，封装代码如下："
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:411
#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1545
msgid "构建 dataset_builder_dict"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:413
#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1547
msgid "我们通过 dataset_builder_dict 为各个参与方传入封装数据处理逻辑的 create_dataset_builder 函数的参数。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:449
msgid "在隐语的框架下定义基于 PyTorch 的模型架构"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:451
msgid "参考 PyTorch 单机模式下的模型，我们在隐语的框架下定义同样结构的模型。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:453
msgid ""
"我们只需修改继承类将 **torch.nn.Module** 改为 "
"**secretflow.ml.nn.core.torch.BaseModule**\\ ，就可以完成模型架构的定义。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:455
msgid "从迁移过程可以看出，将单机模型在隐语框架下进行定义所进行的代码改动非常小，整体迁移非常方便，充分展现了隐语框架的易用性。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:503
msgid "构建TorchModel"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:505
msgid ""
"``TorchModel``\\ 是我们在隐语中定义的一个概念，层级概念上对应\\ ``keras model``\\ 。目的是将将用户定义的\\"
" ``模型``\\ ，\\ ``loss函数``,\\ ``optimizer``,\\ ``metrics``\\ "
"封装在一起，用与后续FLModel在启动后统一多后端逻辑。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:506
msgid "其中需要注意的是，我们的loss_fn，optim_fn，metrics都需要通过一个wrapper封装成一个偏函数传入进去，在后面会有详细的解析。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:585
msgid "定义FLModel"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:977
msgid "构建完成"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:988
msgid "得到FLModel对象之后，就可以用这个对象进行\\ ``fit``,\\ ``evaluate``,\\ ``predict``\\ 等操作了。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1000
msgid "Why need wraps"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1012
msgid "将单机模型下的 optimizer 包装（wrap）"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1014
msgid ""
"对 optimizer 进行包装的原因：我们需要使用 optimizer "
"在训练过程中自动地调整模型参数，以使模型在给定的训练数据上达到最佳的性能表现。同样地，联邦学习也可以通过 optimizer "
"实现模型更好的性能。但由于隐语的设计机制需要把相关模块通过序列化到具体的机器上才会执行，因此在优化器需要指定参数时，需要做一次封装。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1016
msgid "通过\\ ``optim_wrapper``\\ 进行优化器（optimizer）的包装，并且通过追根溯源，我们可以看到。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1022
#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1080
msgid ""
"`source code "
"<https://github.com/secretflow/secretflow/blob/main/secretflow/ml/nn/core/torch/utils.py#L23>`__"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1048
msgid ""
"可以看到函数实际上都是通过传入一个需要包装的函数名称，位置参数和关键字参数对函数完成包装，通过关键字参数确保了指定的参数赋值，然后返回包装好的函数。"
" 因此"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1054
msgid "实际上相当于调用"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1070
msgid "将单机模型下的 metric 包装（wrap）"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1072
msgid ""
"对 metric 进行包装的原因:metric "
"在机器学习和深度学习中用于衡量模型的性能和表现，它们是评估模型在训练、验证或测试数据上的效果的标准。同样地，我们也希望使用 metric "
"衡量联邦学习模型的性能和表现。但由于隐语的设计机制同 PyTorch 有些差异，因此在衡量指标需要指定参数时，需要做一次封装来保证两者的一致性。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1074
msgid "通过\\ ``metric_wrapper``\\ 进行衡量指标（metric）的包装，并且通过追根溯源，我们可以看到"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1105
msgid ""
"可以看到函数实际上是通过传入一个需要包装的函数名称，位置参数和关键字参数对函数完成包装，通过关键字参数确保了指定的参数赋值，然后返回包装好的函数；与优化器包装类似"
" 因此"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1111
msgid ""
"实际上相当于调用 \\```python Accuracy(task=“multiclass”, num_classes=10, "
"average=‘micro’)"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1123
msgid "将单机模型下的 loss function 包装（wrap）"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1125
msgid ""
"参考 ``optim_wrapper`` 和 ``metric_wrapper`` 的定义方式，自定义 "
"``loss_function_wrapper``\\ 对损失函数（loss function）进行包装。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1127
msgid "如果模型使用损失函数的默认参数，则不需要使用包装"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1152
msgid ""
"得益于隐语的封装，并且根据前述的包装原理可知，我们实际上需要通过包装完成一个参数具体化的函数，所以在这里 "
"``nn.CrossEntropyLoss`` 是需要包装的函数，并且其参数取值，就是需要传入的参数值。因为在其默认参数取值设置中， "
"``reduction='mean'`` , 此处我们试着将其修改为 ``reduction='sum'``\\ 。包装损失函数只需要写成："
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1174
msgid "对单机模型使用包装（wrap）后的优化器（optimizer）、衡量指标（metric）和损失函数（loss function）"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1176
msgid ""
"在隐语框架下，使用者可以根据需要选择是否对优化器（optimizer）、衡量指标（metric）和损失函数（loss "
"function）进行包装，从而更好地训练联邦学习模型，充分发挥隐语框架的灵活性。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1188
#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:2045
msgid "小结"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1192
msgid "得益于隐语的封装，使用者不需要自己完成模型定义等代码的编写，只需要调用 Secretflow 中的函数，即可便捷完成模型的定义和使用等操作。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1204
msgid "基于 TensorFlow 的迁移教程"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1207
msgid "模型在 TensorFlow 下的单机模型实现"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1209
msgid ""
"首先我们给出单机模式下，基于 TensorFlow 定义和训练神经网络模型的过程。对于数据，我们将其加载成 TensorFlow 的 "
"dataset 对象"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1381
msgid "基于 TensorFlow 的隐语联邦学习模型迁移"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1384
msgid "概述"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1386
msgid "基于 TensorFlow 从单机模型到联邦学习模型，主要包含以下步骤： - 添加联邦学习中的参与方 - 修改数据集的处理逻辑 - 进行模型的封装"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1388
msgid "接下来，我们将结合实际代码具体讲解这些步骤。 ### 环境设置 添加联邦学习中的参与方，并初始化各个参与方"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1576
msgid "在隐语的框架下定义基于 TensorFlow 的模型架构"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1578
msgid "参考 TensorFlow 单机模式下的模型，我们在隐语的框架下定义同样结构的模型。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1580
msgid ""
"如前所述，我们需要使用 optimizer 在训练过程中自动地调整模型参数，以使模型在给定的训练数据上达到最佳的性能表现；使用 metric "
"在机器学习和深度学习中用于衡量模型的性能和表现；使用损失函数监督神经网络的训练。同样地，联邦学习也可以通过使用优化器（optimizer）、衡量指标（metric）和损失函数（loss"
" function） 实现模型更好的性能。在隐语框架下，优化器（optimizer）、衡量指标（metric）和损失函数（loss "
"function）的使用方法和 TensorFlow 中的使用方法一致，也同样通过模型的 compile 函数实现。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1582
msgid "从迁移过程可以看出，代码修改幅度非常小，整体迁移过程非常方便，充分展现了隐语框架的易用性和便捷性。"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1711
msgid "训练和验证模型"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:1713
msgid "传入参与方的数据集路径，进行模型的训练和验证"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:2061
msgid "总结"
msgstr ""

#: ../../tutorial/Transforming_SingleModel_to_SecretFlow_Federated_Learning.ipynb:2063
msgid ""
"本教程说明了使用者能够在 SecretFlow 隐语的框架下，体会到和单机模式下，使用 PyTorch 或 Tensorflow "
"编程几乎一致的联邦学习模型使用体验。充分展现了隐语框架具有易用性和便捷性等优点。"
msgstr ""
